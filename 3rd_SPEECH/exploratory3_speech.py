# -*- coding: utf-8 -*-
"""Exploratory3-speech

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mJnGBDBWWuXa4GYwPLybUYW4sm4GZ6FD
"""

import numpy as np
import matplotlib.pyplot as plt

def single_tone(frequency, sampling_rate = 16000, duration = 1):
  t = np.linspace(0, duration, int(sampling_rate))
  y = np.sin(2* np.pi * frequency * t)
  return y

y = single_tone(400)

plt.plot(y[:41])
plt.show()

import numpy as np
import tensorflow as tf
data_url = 'https://aiffelstaticdev.blob.core.windows.net/dataset/speech_wav_8000.npz'
path = tf.keras.utils.get_file('speech_wav_8000.npz', data_url)
speech_data = np.load(path)

"""# 새 섹션

# 새 섹션
"""

print("Wave data shape : ", speech_data["wav_vals"].shape)
print("Label data shape : ", speech_data["label_vals"].shape)

import IPython.display as ipd
import random
# 데이터 선택 (랜덤하게 선택하고 있으니, 여러번 실행해 보세요)
rand = random.randint(0, len(speech_data["wav_vals"]))
print("rand num : ", rand)
sr = 8000 # 1초동안 재생되는 샘플의 갯수
data = speech_data["wav_vals"][rand]
print("Wave data shape : ", data.shape)
print("label : ", speech_data["label_vals"][rand])
ipd.Audio(data, rate=sr)

target_list = ['yes', 'no', 'up','down','left', 'right', 'on','off','stop','go']

label_value = target_list
label_value.append('unknown')
label_value.append('silence')

print('LABEL : ', label_value)

new_label_value = dict()

for i, l in enumerate(label_value):
  new_label_value[l] = i
label_value = new_label_value

print('Indexed LABEL : ', new_label_value)

temp = []
for v in speech_data['label_vals']:
  temp.append(label_value[v[0]])
label_data = np.array(temp)

label_data

from sklearn.model_selection import train_test_split
sr = 8000
train_wav, test_wav, train_label, test_label = train_test_split(
    speech_data['wav_vals'],
    label_data,
    test_size = 0.1, 
    shuffle = True
)
print(train_wav)

train_wav = train_wav.reshape([-1, sr, 1]) # add channel for CNN
test_wav = test_wav.reshape([-1, sr, 1])
print('check')

batch_size = 32
max_epochs = 10

# the save point
import os
os.mkdir('models')
checkpoint_dir = './models.wav'
checkpoint_dir

def one_hot_label(wav, label):
  label = tf.one_hot(label, depth=12)
  return wav, label
print('check')

import tensorflow as tf 

# for train
train_dataset = tf.data.Dataset.from_tensor_slices((train_wav, train_label))
train_dataset = train_dataset.map(one_hot_label)
train_dataset = train_dataset.repeat().batch(batch_size = batch_size)
print(train_dataset)

# for test
test_dataset = tf.data.Dataset.from_tensor_slices((test_wav, test_label))
test_dataset = test_dataset.map(one_hot_label)
test_dataset = test_dataset.batch(batch_size = batch_size)
print(test_dataset)
print('check')

from tensorflow.keras import layers
input_tensor = layers.Input(shape = (sr, 1))

x = layers.Conv1D(32, 9, padding = 'same', activation = 'relu')(input_tensor)
x = layers.Conv1D(32, 9, padding = 'same', activation = 'relu')(x)
x = layers.MaxPool1D()(x)

x = layers.Conv1D(64, 9, padding = 'same', activation = 'relu')(x)
x = layers.Conv1D(64, 9, padding = 'same', activation = 'relu')(x)
x = layers.MaxPool1D()(x)

x = layers.Conv1D(128, 9, padding = 'same', activation = 'relu')(x)
x = layers.Conv1D(128, 9, padding = 'same', activation = 'relu')(x)
x = layers.Conv1D(128, 9, padding = 'same', activation = 'relu')(x)
x = layers.MaxPool1D()(x)

x = layers.Conv1D(256, 9, padding = 'same', activation = 'relu')(x)
x = layers.Conv1D(128, 9, padding = 'same', activation = 'relu')(x)
x = layers.Conv1D(128, 9, padding = 'same', activation = 'relu')(x)
x = layers.MaxPool1D()(x)
x = layers.Dropout(0.3)(x)

x = layers.Flatten()(x)
x = layers.Dense(256)(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)

output_tensor = layers.Dense(12)(x)
model_wav = tf.keras.Model(input_tensor, output_tensor)
model_wav.summary()

optimizer = tf.keras.optimizers.Adam(1e-4)
model_wav.compile(loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True), optimizer = optimizer, metrics = 'accuracy')
print('check')

cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,
                                                 save_weights_only = True,
                                                 monitor = 'val_loss',
                                                 mode = 'auto',
                                                 save_best_only = True,
                                                 verbose = 1)
print('check')

# 30분 소요
history_wav = model_wav.fit(train_dataset, epochs = max_epochs,
                            steps_per_epoch = len(train_wav) // batch_size,
                            validation_data = test_dataset,
                            validation_steps = len(test_wav) // batch_size,
                            callbacks = [cp_callback])
print('check')

import matplotlib.pyplot as plt

acc = history_wav.history['accuracy']
val_acc = history_wav.history['val_accuracy']

loss = history_wav.history['loss']
val_loss = history_wav.history['val_loss']

epochs_range = range(len(acc))

plt.figure(figsize = (8, 8))
plt.subplot(121)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label = 'Validation Accuracy')
plt.legend(loc = 'lowr right')
plt.title('Training and Validation Accuracy')

plt.subplot(122)
plt.plot(epochs_range, loss, label = 'Training Loss')
plt.plot(epochs_range, val_loss, label = 'Validation Loss')
plt.legend(loc = 'upper right')
plt.title('Training and Validation Loss')
plt.show()
print('check')

model_wav.load_weights(checkpoint_dir)
print('check')

results = model_wav.evaluate(test_dataset)
print('check')

# loss
print('loss value : {:.3f}'.format(results[0]))
# accuracy
print('accuracy value : {:.4f}%'.format(results[1]*100))
print('check')

inv_label_value = {v: k for k, v in label_value.items()}
batch_index = np.random.choice(len(test_wav), size = 1, replace = False)

batch_xs = test_wav[batch_index]
batch_ys = test_label[batch_index]
y_pred_ = model_wav(batch_xs, training=False)

print('label : ', str(inv_label_value[batch_ys[0]]))
ipd.Audio(batch_xs.reshape(8000, ), rate = 8000)

if np.argmax(y_pred_) == batch_ys[0]:
  print('y_pred: '+ str(inv_label_value[np.argmax(y_pred_)]) + '(Correct!)')
else:
  print('y_pred: '+ str(inv_label_value[np.argmax(y_pred_)]) + '(Incorrect!)')
print('check')

input_tensor = layers.Input(shape=(sr, 1))
x = layers.Conv1D(32, 9, padding='same', activation='relu')(input_tensor)
x = layers.Conv1D(32, 9, padding='same', activation='relu')(x)
skip_1 = layers.MaxPool1D()(x)
x = layers.Conv1D(64, 9, padding='same', activation='relu')(skip_1)
x = layers.Conv1D(64, 9, padding='same', activation='relu')(x)
x = tf.concat([x, skip_1], -1)
skip_2 = layers.MaxPool1D()(x)
x = layers.Conv1D(128, 9, padding='same', activation='relu')(skip_2)
x = layers.Conv1D(128, 9, padding='same', activation='relu')(x)
x = layers.Conv1D(128, 9, padding='same', activation='relu')(x)
x = tf.concat([x, skip_2], -1)
skip_3 = layers.MaxPool1D()(x)
x = layers.Conv1D(256, 9, padding='same', activation='relu')(skip_3)
x = layers.Conv1D(256, 9, padding='same', activation='relu')(x)
x = layers.Conv1D(256, 9, padding='same', activation='relu')(x)
x = tf.concat([x, skip_3], -1)
x = layers.MaxPool1D()(x)
x = layers.Dropout(0.3)(x)
x = layers.Flatten()(x)
x = layers.Dense(256)(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
output_tensor = layers.Dense(12)(x)
model_wav_skip = tf.keras.Model(input_tensor, output_tensor)
model_wav_skip.summary()

optimizer=tf.keras.optimizers.Adam(1e-4)
model_wav_skip.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),

optimizer=optimizer,
metrics=['accuracy'])

print("check")

# the save point
checkpoint_dir = os.getenv('HOME')+'/aiffel/speech_recognition/models/wav_skip'
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,save_weights_only=True,
monitor='val_loss',
mode='auto',
save_best_only=True,
verbose=1)

print("check")

#30분 내외 소요
history_wav_skip = model_wav_skip.fit(train_dataset, epochs=max_epochs,
steps_per_epoch=len(train_wav) // batch_size,
validation_data=test_dataset,
validation_steps=len(test_wav) // batch_size,
callbacks=[cp_callback]
)
print("check")

import matplotlib.pyplot as plt
acc = history_wav_skip.history['accuracy']
val_acc = history_wav_skip.history['val_accuracy']
loss=history_wav_skip.history['loss']
val_loss=history_wav_skip.history['val_loss']
epochs_range = range(len(acc))
plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
print("check")

# Evaluation
model_wav_skip.load_weights(checkpoint_dir)
results = model_wav_skip.evaluate(test_dataset)
# loss
print("loss value: {:.3f}".format(results[0]))
# accuracy
print("accuracy value: {:.4f}%".format(results[1]*100))
print("check")

# Test
inv_label_value = {v: k for k, v in label_value.items()}
batch_index = np.random.choice(len(test_wav), size=1, replace=False)
batch_xs = test_wav[batch_index]
batch_ys = test_label[batch_index]
y_pred_ = model_wav_skip(batch_xs, training=False)
print("label : ", str(inv_label_value[batch_ys[0]]))
ipd.Audio(batch_xs.reshape(8000,), rate=8000)

if np.argmax(y_pred_) == batch_ys[0]:
  print("y_pred: " + str(inv_label_value[np.argmax(y_pred_)]) + '(Correct!)')
else:
  print("y_pred: " + str(inv_label_value[np.argmax(y_pred_)]) + '(Incorrect!)')
print("check")

import librosa
def wav2spec(wav, fft_size=258): # spectrogram shape을 맞추기위해서 size 변형
  D = np.abs(librosa.stft(wav, n_fft=fft_size))
  return D
print("check")

# 위에서 뽑았던 sample data
spec = wav2spec(data)
print("Waveform shape : ",data.shape)
print("Spectrogram shape : ",spec.shape)

import librosa.display
librosa.display.specshow(librosa.amplitude_to_db(spec, ref=np.max), x_axis='time')
plt.title('Power spectrogram')
plt.colorbar(format='%+2.0f dB')
plt.xticks(range(0, 1))
plt.tight_layout()
plt.show()

del speech_data
del spec_data

